{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter \n",
    "from sklearn import preprocessing\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import codecs\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>قام مسئولو اتحاد الكرة بتغيير اللوجو الخاص بال...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>أعلن الاتحاد الأفريقي لكرة القدم، كاف، عن تفاص...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>أعلن نادى سيمبا التنزانى تعيين الفرنسى ديديه ج...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>حرص الجهاز الطبي للفريق الأول لكرة القدم بالنا...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>دائمًا ما تشهد غرفة خلع ملابس الأندية العديد م...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5994</th>\n",
       "      <td>كشفت شركة أمازون خلال سبتمبر الماضى عن نظام دف...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>على ما يبدو أن الهند ستكون الدولة التالية التي...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>نسمع عن مصطلح الطيار الآلي، ولا نعلم كيف تعمل ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>يمكن مشاركة تغريدة من خلال تضمينها في موقع ويب...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>إذا أغلقت العمل الذى تقوم به على مستند الـ Wor...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article   label\n",
       "0     قام مسئولو اتحاد الكرة بتغيير اللوجو الخاص بال...  sports\n",
       "1     أعلن الاتحاد الأفريقي لكرة القدم، كاف، عن تفاص...  sports\n",
       "2     أعلن نادى سيمبا التنزانى تعيين الفرنسى ديديه ج...  sports\n",
       "3     حرص الجهاز الطبي للفريق الأول لكرة القدم بالنا...  sports\n",
       "4     دائمًا ما تشهد غرفة خلع ملابس الأندية العديد م...  sports\n",
       "...                                                 ...     ...\n",
       "5994  كشفت شركة أمازون خلال سبتمبر الماضى عن نظام دف...    tech\n",
       "5995  على ما يبدو أن الهند ستكون الدولة التالية التي...    tech\n",
       "5996  نسمع عن مصطلح الطيار الآلي، ولا نعلم كيف تعمل ...    tech\n",
       "5997  يمكن مشاركة تغريدة من خلال تضمينها في موقع ويب...    tech\n",
       "5998  إذا أغلقت العمل الذى تقوم به على مستند الـ Wor...    tech\n",
       "\n",
       "[5999 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"data.csv\")\n",
    "dataset.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sports', 'economics', 'politics', 'celebrity', 'tech'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "dataset[\"article\"] = dataset[\"article\"].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[قام, مسئولو, اتحاد, الكرة, بتغيير, اللوجو, ال...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[أعلن, الاتحاد, الأفريقي, لكرة, القدم, كاف, عن...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[أعلن, نادى, سيمبا, التنزانى, تعيين, الفرنسى, ...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[حرص, الجهاز, الطبي, للفريق, الأول, لكرة, القد...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[دائم, ا, ما, تشهد, غرفة, خلع, ملابس, الأندية,...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article   label\n",
       "0  [قام, مسئولو, اتحاد, الكرة, بتغيير, اللوجو, ال...  sports\n",
       "1  [أعلن, الاتحاد, الأفريقي, لكرة, القدم, كاف, عن...  sports\n",
       "2  [أعلن, نادى, سيمبا, التنزانى, تعيين, الفرنسى, ...  sports\n",
       "3  [حرص, الجهاز, الطبي, للفريق, الأول, لكرة, القد...  sports\n",
       "4  [دائم, ا, ما, تشهد, غرفة, خلع, ملابس, الأندية,...  sports"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading arabic stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words(path):\n",
    "    stop_words = []\n",
    "    with codecs.open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as myfile:\n",
    "        stop_words = myfile.readlines()\n",
    "    stop_words = [word.strip() for word in stop_words]\n",
    "    return stop_words\n",
    "stop_words = get_stop_words('Arabic_stop_word.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"article\"]=dataset[\"article\"].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [مسئولو, اتحاد, الكرة, بتغيير, اللوجو, الخاص, ...\n",
       "1    [أعلن, الاتحاد, الأفريقي, لكرة, القدم, تفاصيل,...\n",
       "2    [أعلن, نادى, سيمبا, التنزانى, تعيين, الفرنسى, ...\n",
       "3    [حرص, الجهاز, الطبي, للفريق, الأول, لكرة, القد...\n",
       "4    [دائم, تشهد, غرفة, خلع, ملابس, الأندية, العديد...\n",
       "Name: article, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['article'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing the most common data in every label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('الزمالك', 344),\n",
       " ('الفريق', 281),\n",
       " ('العالم', 234),\n",
       " ('لكرة', 231),\n",
       " ('مصر', 209),\n",
       " ('الأهلي', 187),\n",
       " ('اليد', 185),\n",
       " ('محمد', 177),\n",
       " ('فريق', 154),\n",
       " ('المدير', 152),\n",
       " ('الأول', 147),\n",
       " ('الكرة', 146),\n",
       " ('كأس', 146),\n",
       " ('مباراة', 139),\n",
       " ('الأهلى', 139),\n",
       " ('بطولة', 131),\n",
       " ('لاعب', 128),\n",
       " ('منتخب', 127),\n",
       " ('الفني', 115),\n",
       " ('مباريات', 107),\n",
       " ('القدم', 106),\n",
       " ('للأندية', 106),\n",
       " ('مهاجم', 104),\n",
       " ('نادى', 103),\n",
       " ('كشف', 102)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_sports = [word for tokens in dataset[dataset['label']=='sports']['article'] for word in tokens]\n",
    "counter = Counter(all_words_sports)\n",
    "counter.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2021', 289),\n",
       " ('1', 270),\n",
       " ('نقطة', 262),\n",
       " ('تعاملات', 253),\n",
       " ('العام', 250),\n",
       " ('مصر', 197),\n",
       " ('أسعار', 188),\n",
       " ('جلسة', 183),\n",
       " ('المصرية', 178),\n",
       " ('المصرى', 174),\n",
       " ('بنسبة', 163),\n",
       " ('مستوى', 154),\n",
       " ('0', 147),\n",
       " ('رئيس', 143),\n",
       " ('سعر', 140),\n",
       " ('الأسبوع', 136),\n",
       " ('جلسات', 136),\n",
       " ('الأربعاء', 128),\n",
       " ('الثلاثاء', 127),\n",
       " ('المؤشر', 118),\n",
       " ('وذلك', 118),\n",
       " ('التجارية', 107),\n",
       " ('بختام', 104),\n",
       " ('الخميس', 98),\n",
       " ('الذهب', 96)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_economics = [word for tokens in dataset[dataset['label']=='economics']['article'] for word in tokens]\n",
    "counter = Counter(all_words_economics)\n",
    "counter.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('النواب', 689),\n",
       " ('مجلس', 364),\n",
       " ('لجنة', 322),\n",
       " ('رئيس', 305),\n",
       " ('بمجلس', 284),\n",
       " ('وزير', 249),\n",
       " ('الدكتور', 216),\n",
       " ('النائب', 213),\n",
       " ('برئاسة', 198),\n",
       " ('العامة', 194),\n",
       " ('قانون', 194),\n",
       " ('المستشار', 192),\n",
       " ('عبد', 142),\n",
       " ('محمد', 128),\n",
       " ('رقم', 120),\n",
       " ('لسنة', 119),\n",
       " ('الجلسة', 118),\n",
       " ('عضو', 117),\n",
       " ('الحكومة', 115),\n",
       " ('اللجنة', 113),\n",
       " ('جبالى', 105),\n",
       " ('حنفى', 104),\n",
       " ('مصر', 101),\n",
       " ('أكد', 99),\n",
       " ('المجلس', 94)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_politics = [word for tokens in dataset[dataset['label']=='politics']['article'] for word in tokens]\n",
    "counter = Counter(all_words_politics)\n",
    "counter.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('الفنانة', 224),\n",
       " ('الفنان', 222),\n",
       " ('النجم', 222),\n",
       " ('النجمة', 214),\n",
       " ('محمد', 198),\n",
       " ('عبر', 155),\n",
       " ('مسلسل', 128),\n",
       " ('فيلم', 103),\n",
       " ('الجديد', 100),\n",
       " ('أحمد', 97),\n",
       " ('تصوير', 95),\n",
       " ('موقع', 95),\n",
       " ('عبد', 86),\n",
       " ('صورة', 78),\n",
       " ('الصغير', 78),\n",
       " ('العالمية', 74),\n",
       " ('كشف', 72),\n",
       " ('جديدة', 69),\n",
       " ('الراحل', 64),\n",
       " ('حسابها', 64),\n",
       " ('كورونا', 64),\n",
       " ('رمضان', 62),\n",
       " ('حسابه', 59),\n",
       " ('السابع', 59),\n",
       " ('انستجرام', 56)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_celebrity = [word for tokens in dataset[dataset['label']=='celebrity']['article'] for word in tokens]\n",
    "counter = Counter(all_words_celebrity)\n",
    "counter.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('شركة', 510),\n",
       " ('الألعاب', 180),\n",
       " ('إيلون', 150),\n",
       " ('ماسك', 150),\n",
       " ('التنفيذي', 150),\n",
       " ('لشركة', 150),\n",
       " ('العام', 150),\n",
       " ('جديدة', 150),\n",
       " ('الخاص', 150),\n",
       " ('قالت', 150),\n",
       " ('الملياردير', 120),\n",
       " ('SpaceX', 120),\n",
       " ('جوجل', 120),\n",
       " ('استخدام', 120),\n",
       " ('تطبيق', 120),\n",
       " ('موقع', 120),\n",
       " ('أعلن', 120),\n",
       " ('إنها', 120),\n",
       " ('ارتداء', 120),\n",
       " ('الطيار', 120),\n",
       " ('التكنولوجي', 90),\n",
       " ('الجديدة', 90),\n",
       " ('Clubhouse', 90),\n",
       " ('كشفت', 90),\n",
       " ('أعلنت', 90)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_tech = [word for tokens in dataset[dataset['label']=='tech']['article'] for word in tokens]\n",
    "counter = Counter(all_words_tech)\n",
    "counter.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer_uni = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words=stop_words,\n",
    "    ngram_range=(1, 1),\n",
    "    max_features =1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigramdataGet= word_vectorizer_uni.fit_transform(dataset['article'].astype('str'))\n",
    "unigramdataGet = unigramdataGet.toarray()\n",
    "vocab = word_vectorizer_uni.get_feature_names()\n",
    "unigramdata=pd.DataFrame(np.round(unigramdataGet, 1), columns=vocab)\n",
    "unigramdata[unigramdata>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer_bi = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words=stop_words,\n",
    "    ngram_range=(2, 2),\n",
    "    max_features =1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiagramdataGet= word_vectorizer_bi.fit_transform(dataset['article'].astype('str'))\n",
    "BiagramdataGet= BiagramdataGet.toarray()\n",
    "\n",
    "vocab1 = word_vectorizer_bi.get_feature_names()\n",
    "Biagramdata=pd.DataFrame(np.round(BiagramdataGet, 1), columns=vocab1)\n",
    "Biagramdata[Biagramdata>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=unigramdata\n",
    "y=dataset['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score= 0.94\n"
     ]
    }
   ],
   "source": [
    "svc=LinearSVC(C=1, max_iter=500)\n",
    "svc= svc.fit(X_train , y_train)\n",
    "\n",
    "y_pred1 = svc.predict(X_test)\n",
    "dm=svc.score(X_test, y_test)\n",
    "print('Accuracy score= {:.2f}'.format(svc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score= 0.90\n"
     ]
    }
   ],
   "source": [
    "nab=GaussianNB(var_smoothing=1e-08)\n",
    "nab= nab.fit(X_train , y_train)\n",
    "\n",
    "y_pred1 = nab.predict(X_test)\n",
    "nb=nab.score(X_test, y_test)\n",
    "print('Accuracy score= {:.2f}'.format(nab.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score= 0.94\n"
     ]
    }
   ],
   "source": [
    "lor=LogisticRegression(C=2, max_iter=100)\n",
    "lor= lor.fit(X_train , y_train)\n",
    "\n",
    "y_pred1 = lor.predict(X_test)\n",
    "lr=lor.score(X_test, y_test)\n",
    "print('Accuracy score= {:.2f}'.format(lor.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score= 0.87\n"
     ]
    }
   ],
   "source": [
    "raf=RandomForestClassifier(min_samples_leaf=20, min_samples_split=20,random_state=100)\n",
    "raf= raf.fit(X_train , y_train)\n",
    "\n",
    "y_pred1 = raf.predict(X_test)\n",
    "rf=raf.score(X_test, y_test)\n",
    "print('Accuracy score= {:.2f}'.format(raf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score= 0.94\n"
     ]
    }
   ],
   "source": [
    "Ens = VotingClassifier( estimators= [('SVM',svc),('NB',nab),('Log',lor),('raf',raf)], voting = 'hard')\n",
    "Ens= Ens.fit(X_train , y_train)\n",
    "\n",
    "y_pred1 = Ens.predict(X_test)\n",
    "en=Ens.score(X_test, y_test)\n",
    "print('Accuracy score= {:.2f}'.format(Ens.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=Biagramdata\n",
    "y=dataset['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score= 0.72\n"
     ]
    }
   ],
   "source": [
    "svc=LinearSVC(C=1, max_iter=500)\n",
    "svc= svc.fit(X_train , y_train)\n",
    "\n",
    "y_pred1 = svc.predict(X_test)\n",
    "dm=svc.score(X_test, y_test)\n",
    "print('Accuracy score= {:.2f}'.format(svc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score= 0.71\n"
     ]
    }
   ],
   "source": [
    "nab=GaussianNB(var_smoothing=1e-08)\n",
    "nab= nab.fit(X_train , y_train)\n",
    "\n",
    "y_pred1 = nab.predict(X_test)\n",
    "nb=nab.score(X_test, y_test)\n",
    "print('Accuracy score= {:.2f}'.format(nab.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score= 0.72\n"
     ]
    }
   ],
   "source": [
    "lor=LogisticRegression(C=2, max_iter=100)\n",
    "lor= lor.fit(X_train , y_train)\n",
    "\n",
    "y_pred1 = lor.predict(X_test)\n",
    "lr=lor.score(X_test, y_test)\n",
    "print('Accuracy score= {:.2f}'.format(lor.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score= 0.56\n"
     ]
    }
   ],
   "source": [
    "raf=RandomForestClassifier(min_samples_leaf=20, min_samples_split=20,random_state=100)\n",
    "raf= raf.fit(X_train , y_train)\n",
    "\n",
    "y_pred1 = raf.predict(X_test)\n",
    "rf=raf.score(X_test, y_test)\n",
    "print('Accuracy score= {:.2f}'.format(raf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score= 0.72\n"
     ]
    }
   ],
   "source": [
    "Ens = VotingClassifier( estimators= [('SVM',svc),('NB',nab),('Log',lor),('raf',raf)], voting = 'hard')\n",
    "Ens= Ens.fit(X_train , y_train)\n",
    "\n",
    "y_pred1 = Ens.predict(X_test)\n",
    "en=Ens.score(X_test, y_test)\n",
    "print('Accuracy score= {:.2f}'.format(Ens.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stop_words('Arabic_stop_word.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"article\"]=dataset[\"article\"].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer_uni = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words=stop_words,\n",
    "    ngram_range=(1, 1),\n",
    "    max_features =1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigramdataGet= word_vectorizer_uni.fit_transform(dataset['article'].astype('str'))\n",
    "unigramdataGet = unigramdataGet.toarray()\n",
    "vocab = word_vectorizer_uni.get_feature_names()\n",
    "unigramdata=pd.DataFrame(np.round(unigramdataGet, 1), columns=vocab)\n",
    "unigramdata[unigramdata>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=unigramdata\n",
    "y=dataset['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score= 0.93\n"
     ]
    }
   ],
   "source": [
    "svc=LinearSVC(C=1, max_iter=500)\n",
    "svc= svc.fit(X_train , y_train)\n",
    "\n",
    "y_pred1 = svc.predict(X_test)\n",
    "dm=svc.score(X_test, y_test)\n",
    "print('Accuracy score= {:.2f}'.format(svc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score= 0.89\n"
     ]
    }
   ],
   "source": [
    "nab=GaussianNB(var_smoothing=1e-08)\n",
    "nab= nab.fit(X_train , y_train)\n",
    "\n",
    "y_pred1 = nab.predict(X_test)\n",
    "nb=nab.score(X_test, y_test)\n",
    "print('Accuracy score= {:.2f}'.format(nab.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score= 0.93\n"
     ]
    }
   ],
   "source": [
    "lor=LogisticRegression(C=2, max_iter=100)\n",
    "lor= lor.fit(X_train , y_train)\n",
    "\n",
    "y_pred1 = lor.predict(X_test)\n",
    "lr=lor.score(X_test, y_test)\n",
    "print('Accuracy score= {:.2f}'.format(lor.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score= 0.86\n"
     ]
    }
   ],
   "source": [
    "raf=RandomForestClassifier(min_samples_leaf=20, min_samples_split=20,random_state=100)\n",
    "raf= raf.fit(X_train , y_train)\n",
    "\n",
    "y_pred1 = raf.predict(X_test)\n",
    "rf=raf.score(X_test, y_test)\n",
    "print('Accuracy score= {:.2f}'.format(raf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score= 0.93\n"
     ]
    }
   ],
   "source": [
    "Ens = VotingClassifier( estimators= [('SVM',svc),('NB',nab),('Log',lor),('raf',raf)], voting = 'hard')\n",
    "Ens= Ens.fit(X_train , y_train)\n",
    "\n",
    "y_pred1 = Ens.predict(X_test)\n",
    "en=Ens.score(X_test, y_test)\n",
    "print('Accuracy score= {:.2f}'.format(Ens.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict (X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=100)\n",
    "    \n",
    "    svc=LinearSVC(C=1, max_iter=500)\n",
    "    svc= svc.fit(X_train , y_train)\n",
    "    y_pred1 = svc.predict(X_test)\n",
    "    dm=svc.score(X_test, y_test)\n",
    "    \n",
    "    nab=GaussianNB(var_smoothing=1e-08)\n",
    "    nab= nab.fit(X_train , y_train)\n",
    "    y_pred2 = nab.predict(X_test)\n",
    "    nb=nab.score(X_test, y_test)\n",
    "    \n",
    "    lor=LogisticRegression(C=2, max_iter=100)\n",
    "    lor= lor.fit(X_train , y_train)\n",
    "    y_pred3 = lor.predict(X_test)\n",
    "    lr=lor.score(X_test, y_test)\n",
    "    \n",
    "    raf=RandomForestClassifier(min_samples_leaf=20, min_samples_split=20,random_state=100)\n",
    "    raf= raf.fit(X_train , y_train)\n",
    "    y_pred4 = raf.predict(X_test)\n",
    "    rf=raf.score(X_test, y_test)\n",
    "    \n",
    "    Ens = VotingClassifier( estimators= [('SVM',svc),('NB',nab),('Log',lor),('raf',raf)], voting = 'hard')\n",
    "    Ens= Ens.fit(X_train , y_train)\n",
    "    y_pred5 = Ens.predict(X_test)\n",
    "    en=Ens.score(X_test, y_test)\n",
    "    \n",
    "    \n",
    "    print('Accuracy score for SVC= {:.2f}'.format(dm))  \n",
    "    print('Accuracy score for NB= {:.2f}'.format(nb))\n",
    "    print('Accuracy score for LR= {:.2f}'.format(lr))\n",
    "    print('Accuracy score for RF= {:.2f}'.format(rf))\n",
    "    print('Accuracy score for EN= {:.2f}'.format(en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping more data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>أكد عادل عقل، الخبير التحكيمى، أن الحكم الجوات...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>كشف محمد أسامة، رئيس الجهاز الطبي بالزمالك، حق...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>أكد طارق قنديل، عضو مجلس إدارة النادى الأهلي ،...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>احتفل مؤمن زكريا، لاعب الأهلي، بقوز المارد الأ...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>وصلت بعثة الفريق الأول لكرة القدم بالنادي الا...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22885</th>\n",
       "      <td>قال الجنرال المتقاعد بالجيش الأمريكي لويد أوست...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22886</th>\n",
       "      <td>قال الرئيس الأمريكى المنتهية ولايته، دونالد تر...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22887</th>\n",
       "      <td>وصل منذ قليل الرئيس الأمريكي المنتخب جو بادين ...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22888</th>\n",
       "      <td>أكد رئيس نيجيريا محمد بخاري اليوم الثلاثاء أهم...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22889</th>\n",
       "      <td>أفادت مسودة وثيقة حكومية بأن المستشارة الألما...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22890 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 article   label\n",
       "0      أكد عادل عقل، الخبير التحكيمى، أن الحكم الجوات...  sports\n",
       "1      كشف محمد أسامة، رئيس الجهاز الطبي بالزمالك، حق...  sports\n",
       "2      أكد طارق قنديل، عضو مجلس إدارة النادى الأهلي ،...  sports\n",
       "3      احتفل مؤمن زكريا، لاعب الأهلي، بقوز المارد الأ...  sports\n",
       "4      وصلت بعثة الفريق الأول لكرة القدم بالنادي الا...  sports\n",
       "...                                                  ...     ...\n",
       "22885  قال الجنرال المتقاعد بالجيش الأمريكي لويد أوست...   world\n",
       "22886  قال الرئيس الأمريكى المنتهية ولايته، دونالد تر...   world\n",
       "22887  وصل منذ قليل الرئيس الأمريكي المنتخب جو بادين ...   world\n",
       "22888  أكد رئيس نيجيريا محمد بخاري اليوم الثلاثاء أهم...   world\n",
       "22889   أفادت مسودة وثيقة حكومية بأن المستشارة الألما...   world\n",
       "\n",
       "[22890 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"data_4.csv\")\n",
    "dataset.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sports', 'economics', 'politics', 'celebrity', 'tech',\n",
       "       'accidents', 'world_sports', 'health', 'arab', 'world'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[أكد, عادل, عقل, الخبير, التحكيمى, أن, الحكم, ...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[كشف, محمد, أسامة, رئيس, الجهاز, الطبي, بالزما...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[أكد, طارق, قنديل, عضو, مجلس, إدارة, النادى, ا...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[احتفل, مؤمن, زكريا, لاعب, الأهلي, بقوز, المار...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[وصلت, بعثة, الفريق, الا, ول, لكرة, القدم, بال...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article   label\n",
       "0  [أكد, عادل, عقل, الخبير, التحكيمى, أن, الحكم, ...  sports\n",
       "1  [كشف, محمد, أسامة, رئيس, الجهاز, الطبي, بالزما...  sports\n",
       "2  [أكد, طارق, قنديل, عضو, مجلس, إدارة, النادى, ا...  sports\n",
       "3  [احتفل, مؤمن, زكريا, لاعب, الأهلي, بقوز, المار...  sports\n",
       "4  [وصلت, بعثة, الفريق, الا, ول, لكرة, القدم, بال...  sports"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "dataset[\"article\"] = dataset[\"article\"].apply(tokenizer.tokenize)\n",
    "dataset = dataset.drop(['Unnamed: 0'], axis=1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"article\"]=dataset[\"article\"].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [عادل, عقل, الخبير, التحكيمى, الحكم, الجواتيما...\n",
       "1    [أسامة, الجهاز, الطبي, بالزمالك, حقيقة, الأنبا...\n",
       "2    [طارق, قنديل, عضو, مجلس, إدارة, النادى, الأهلي...\n",
       "3    [احتفل, مؤمن, زكريا, لاعب, الأهلي, بقوز, المار...\n",
       "4    [وصلت, بعثة, الفريق, ول, لكرة, القدم, بالنادي,...\n",
       "Name: article, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['article'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('الزمالك', 549),\n",
       " ('لكرة', 478),\n",
       " ('العالم', 453),\n",
       " ('الفريق', 452),\n",
       " ('الأهلي', 422),\n",
       " ('اليد', 387),\n",
       " ('الأهلى', 331),\n",
       " ('الكرة', 331),\n",
       " ('فريق', 315),\n",
       " ('مباراة', 293),\n",
       " ('منتخب', 293),\n",
       " ('الأول', 267),\n",
       " ('المدير', 266),\n",
       " ('كأس', 256),\n",
       " ('لاعب', 253),\n",
       " ('بطولة', 249),\n",
       " ('الجولة', 233),\n",
       " ('المباراة', 229),\n",
       " ('الفني', 223),\n",
       " ('الدوري', 221),\n",
       " ('بيراميدز', 220),\n",
       " ('للأندية', 192),\n",
       " ('مباريات', 192),\n",
       " ('الدورى', 181),\n",
       " ('القدم', 180)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_sports = [word for tokens in dataset[dataset['label']=='sports']['article'] for word in tokens]\n",
    "counter = Counter(all_words_sports)\n",
    "counter.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('تعاملات', 476),\n",
       " ('أسعار', 424),\n",
       " ('جلسة', 364),\n",
       " ('المصرية', 351),\n",
       " ('المصرى', 346),\n",
       " ('مستوى', 312),\n",
       " ('بنسبة', 310),\n",
       " ('سعر', 277),\n",
       " ('جلسات', 252),\n",
       " ('المؤشر', 235),\n",
       " ('المالية', 202),\n",
       " ('التجارية', 198),\n",
       " ('الذهب', 186),\n",
       " ('وزير', 185),\n",
       " ('الاثنين', 184),\n",
       " ('للبيع', 181),\n",
       " ('ليغلق', 180),\n",
       " ('للشراء', 179),\n",
       " ('لسوق', 179),\n",
       " ('الدكتور', 175),\n",
       " ('البنك', 167),\n",
       " ('تراجع', 161),\n",
       " ('الأحد', 160),\n",
       " ('الجنيه', 155),\n",
       " ('البنوك', 154)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_economics = [word for tokens in dataset[dataset['label']=='economics']['article'] for word in tokens]\n",
    "counter = Counter(all_words_economics)\n",
    "counter.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('النواب', 1225),\n",
       " ('مجلس', 795),\n",
       " ('لجنة', 384),\n",
       " ('قانون', 377),\n",
       " ('بمجلس', 356),\n",
       " ('النائب', 350),\n",
       " ('المستشار', 339),\n",
       " ('الدكتور', 337),\n",
       " ('العامة', 309),\n",
       " ('وزير', 302),\n",
       " ('عضو', 298),\n",
       " ('برئاسة', 285),\n",
       " ('لسنة', 267),\n",
       " ('رقم', 265),\n",
       " ('المجلس', 238),\n",
       " ('الجلسة', 201),\n",
       " ('لمجلس', 200),\n",
       " ('القانون', 182),\n",
       " ('بشأن', 166),\n",
       " ('الأحزاب', 150),\n",
       " ('شباب', 145),\n",
       " ('اللجنة', 143),\n",
       " ('حزب', 142),\n",
       " ('الحكومة', 141),\n",
       " ('جبالى', 140)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_politics = [word for tokens in dataset[dataset['label']=='politics']['article'] for word in tokens]\n",
    "counter = Counter(all_words_politics)\n",
    "counter.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('الفنان', 471),\n",
       " ('الفنانة', 435),\n",
       " ('النجمة', 401),\n",
       " ('النجم', 387),\n",
       " ('مسلسل', 260),\n",
       " ('فيلم', 195),\n",
       " ('أحمد', 177),\n",
       " ('موقع', 174),\n",
       " ('تصوير', 167),\n",
       " ('الجديد', 162),\n",
       " ('صورة', 148),\n",
       " ('جديدة', 135),\n",
       " ('أغنية', 126),\n",
       " ('حسابها', 123),\n",
       " ('العالمية', 120),\n",
       " ('المطرب', 111),\n",
       " ('الراحل', 107),\n",
       " ('الكبير', 105),\n",
       " ('إنستجرام', 103),\n",
       " ('عمر', 102),\n",
       " ('رمضان', 102),\n",
       " ('حسابه', 102),\n",
       " ('انستجرام', 100),\n",
       " ('بعيد', 98),\n",
       " ('بموقع', 86)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_celebrity = [word for tokens in dataset[dataset['label']=='celebrity']['article'] for word in tokens]\n",
    "counter = Counter(all_words_celebrity)\n",
    "counter.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('شركة', 571),\n",
       " ('أبل', 316),\n",
       " ('جوجل', 211),\n",
       " ('جديدة', 170),\n",
       " ('الفضاء', 156),\n",
       " ('تطبيق', 152),\n",
       " ('العالم', 139),\n",
       " ('الجديدة', 131),\n",
       " ('بوك', 124),\n",
       " ('الشركة', 117),\n",
       " ('لشركة', 114),\n",
       " ('أكثر', 113),\n",
       " ('جديد', 111),\n",
       " ('الأمريكية', 109),\n",
       " ('iPhone', 108),\n",
       " ('فيس', 105),\n",
       " ('الرئيس', 102),\n",
       " ('المتحدة', 99),\n",
       " ('الخاصة', 96),\n",
       " ('المستخدمين', 93),\n",
       " ('تقرير', 92),\n",
       " ('مجموعة', 92),\n",
       " ('الهواتف', 91),\n",
       " ('موقع', 90),\n",
       " ('الذكية', 90)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_tech = [word for tokens in dataset[dataset['label']=='tech']['article'] for word in tokens]\n",
    "counter = Counter(all_words_tech)\n",
    "counter.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('محكمة', 332),\n",
       " ('العامة', 327),\n",
       " ('القاهرة', 308),\n",
       " ('الأمن', 251),\n",
       " ('أمن', 242),\n",
       " ('النيابة', 242),\n",
       " ('الجيزة', 234),\n",
       " ('ضبط', 220),\n",
       " ('قررت', 199),\n",
       " ('أجهزة', 194),\n",
       " ('الأمنية', 190),\n",
       " ('الداخلية', 185),\n",
       " ('رجال', 184),\n",
       " ('ذمة', 181),\n",
       " ('نيابة', 180),\n",
       " ('اللواء', 172),\n",
       " ('جنايات', 171),\n",
       " ('داخل', 163),\n",
       " ('برئاسة', 158),\n",
       " ('بمديرية', 157),\n",
       " ('حبس', 155),\n",
       " ('المستشار', 153),\n",
       " ('بمحافظة', 152),\n",
       " ('شرطة', 145),\n",
       " ('التحقيقات', 139)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_accidents = [word for tokens in dataset[dataset['label']=='accidents']['article'] for word in tokens]\n",
    "counter = Counter(all_words_accidents)\n",
    "counter.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('فريق', 780),\n",
       " ('ملعب', 523),\n",
       " ('الدوري', 409),\n",
       " ('المباراة', 382),\n",
       " ('كأس', 369),\n",
       " ('الإنجليزي', 314),\n",
       " ('منافسات', 299),\n",
       " ('ليفربول', 295),\n",
       " ('نظيره', 269),\n",
       " ('النجم', 267),\n",
       " ('الجولة', 266),\n",
       " ('يونايتد', 257),\n",
       " ('مانشستر', 253),\n",
       " ('المدير', 243),\n",
       " ('مواجهة', 230),\n",
       " ('العالم', 224),\n",
       " ('لفريق', 216),\n",
       " ('مضيفه', 212),\n",
       " ('برشلونة', 209),\n",
       " ('الفريقين', 206),\n",
       " ('مدريد', 202),\n",
       " ('لاعب', 201),\n",
       " ('القدم', 193),\n",
       " ('نادي', 182),\n",
       " ('الإسباني', 177)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_world_sports = [word for tokens in dataset[dataset['label']=='world_sports']['article'] for word in tokens]\n",
    "counter = Counter(all_words_world_sports)\n",
    "counter.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('فيروس', 625),\n",
       " ('لقاح', 345),\n",
       " ('دراسة', 325),\n",
       " ('الأمريكية', 239),\n",
       " ('جديدة', 194),\n",
       " ('الإصابة', 179),\n",
       " ('أكثر', 177),\n",
       " ('المتحدة', 166),\n",
       " ('الصحة', 148),\n",
       " ('لفيروس', 145),\n",
       " ('الأمراض', 142),\n",
       " ('بفيروس', 138),\n",
       " ('العالم', 136),\n",
       " ('الصحية', 134),\n",
       " ('شركة', 133),\n",
       " ('تقرير', 131),\n",
       " ('البريطانية', 125),\n",
       " ('جامعة', 125),\n",
       " ('الأشخاص', 107),\n",
       " ('العديد', 100),\n",
       " ('موقع', 99),\n",
       " ('الأطفال', 97),\n",
       " ('الجسم', 95),\n",
       " ('الجديدة', 91),\n",
       " ('فايزر', 90)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_health = [word for tokens in dataset[dataset['label']=='health']['article'] for word in tokens]\n",
    "counter = Counter(all_words_health)\n",
    "counter.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('وزارة', 399),\n",
       " ('العربية', 372),\n",
       " ('الصحة', 349),\n",
       " ('بفيروس', 282),\n",
       " ('إصابة', 275),\n",
       " ('وزير', 262),\n",
       " ('جديدة', 254),\n",
       " ('عاجل', 230),\n",
       " ('قناة', 225),\n",
       " ('مجلس', 221),\n",
       " ('قليل', 217),\n",
       " ('تسجيل', 217),\n",
       " ('السعودية', 216),\n",
       " ('المستجد', 205),\n",
       " ('حالة', 199),\n",
       " ('وفقا', 173),\n",
       " ('الخارجية', 172),\n",
       " ('الوزراء', 170),\n",
       " ('المتحدة', 160),\n",
       " ('فيروس', 142),\n",
       " ('لخبر', 134),\n",
       " ('الماضية', 134),\n",
       " ('بثته', 133),\n",
       " ('الدكتور', 131),\n",
       " ('الحكومة', 122)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_arab = [word for tokens in dataset[dataset['label']=='arab']['article'] for word in tokens]\n",
    "counter = Counter(all_words_arab)\n",
    "counter.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('الرئيس', 572),\n",
       " ('بايدن', 369),\n",
       " ('المتحدة', 357),\n",
       " ('الأمريكي', 332),\n",
       " ('جو', 332),\n",
       " ('الأمريكية', 319),\n",
       " ('فيروس', 293),\n",
       " ('بفيروس', 252),\n",
       " ('جديدة', 251),\n",
       " ('المستجد', 219),\n",
       " ('إصابة', 211),\n",
       " ('وفقا', 201),\n",
       " ('وزارة', 201),\n",
       " ('الولايات', 192),\n",
       " ('الصحة', 191),\n",
       " ('الأمريكى', 184),\n",
       " ('وزير', 178),\n",
       " ('الخارجية', 175),\n",
       " ('قناة', 173),\n",
       " ('عاجل', 170),\n",
       " ('كوفيد', 164),\n",
       " ('البلاد', 163),\n",
       " ('العربية', 162),\n",
       " ('قليل', 157),\n",
       " ('ترامب', 155)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_world = [word for tokens in dataset[dataset['label']=='world']['article'] for word in tokens]\n",
    "counter = Counter(all_words_world)\n",
    "counter.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer_uni = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words=stop_words,\n",
    "    ngram_range=(1, 1),\n",
    "    max_features =1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigramdataGet= word_vectorizer_uni.fit_transform(dataset['article'].astype('str'))\n",
    "unigramdataGet = unigramdataGet.toarray()\n",
    "vocab = word_vectorizer_uni.get_feature_names()\n",
    "unigramdata=pd.DataFrame(np.round(unigramdataGet, 1), columns=vocab)\n",
    "unigramdata[unigramdata>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score= 0.83\n",
      "Accuracy score= 0.71\n",
      "Accuracy score= 0.84\n",
      "Accuracy score= 0.71\n",
      "Accuracy score= 0.83\n"
     ]
    }
   ],
   "source": [
    "X=unigramdata\n",
    "y=dataset['label']\n",
    "\n",
    "train_predict(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stop_words('Arabic_stop_word.txt')\n",
    "dataset[\"article\"]=dataset[\"article\"].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer_uni = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words=stop_words,\n",
    "    ngram_range=(1, 1),\n",
    "    max_features =1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigramdataGet= word_vectorizer_uni.fit_transform(dataset['article'].astype('str'))\n",
    "unigramdataGet = unigramdataGet.toarray()\n",
    "vocab = word_vectorizer_uni.get_feature_names()\n",
    "unigramdata=pd.DataFrame(np.round(unigramdataGet, 1), columns=vocab)\n",
    "unigramdata[unigramdata>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for SVC= 0.81\n",
      "Accuracy score for NB= 0.70\n",
      "Accuracy score for LR= 0.82\n",
      "Accuracy score for RF= 0.72\n",
      "Accuracy score for EN= 0.81\n"
     ]
    }
   ],
   "source": [
    "X=unigramdata\n",
    "y=dataset['label']\n",
    "\n",
    "train_predict(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[عادل, عقل, الخبير, التحكيمى, الحكم, الجواتيما...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[أسامة, الجهاز, الطبي, بالزمالك, حقيقة, الأنبا...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[طارق, قنديل, عضو, مجلس, إدارة, النادى, الأهلي...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[احتفل, مؤمن, زكريا, الأهلي, بقوز, المارد, الأ...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[وصلت, بعثة, الفريق, ول, لكرة, بالنادي, هلي, ل...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article   label\n",
       "0  [عادل, عقل, الخبير, التحكيمى, الحكم, الجواتيما...  sports\n",
       "1  [أسامة, الجهاز, الطبي, بالزمالك, حقيقة, الأنبا...  sports\n",
       "2  [طارق, قنديل, عضو, مجلس, إدارة, النادى, الأهلي...  sports\n",
       "3  [احتفل, مؤمن, زكريا, الأهلي, بقوز, المارد, الأ...  sports\n",
       "4  [وصلت, بعثة, الفريق, ول, لكرة, بالنادي, هلي, ل...  sports"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"data_4.csv\")\n",
    "dataset.drop(['Unnamed: 0'], axis=1)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "dataset[\"article\"] = dataset[\"article\"].apply(tokenizer.tokenize)\n",
    "dataset = dataset.drop(['Unnamed: 0'], axis=1)\n",
    "dataset[\"article\"]=dataset[\"article\"].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[عدل, عقل, خبر, حكيمى, حكم, جواتيمالى, ظهر, ست...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[سمة, جهز, طبي, زمل, حقق, باء, دول, فتر, صبة, ...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[طرق, قنديل, عضو, جلس, درة, ندى, اهل, نهم, يوج...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[حفل, ؤمن, زكر, اهل, بقز, ارد, حمر, دحل, قطر, ...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[وصل, بعث, فرق, ول, لكر, ندي, هلي, لى, مقر, قم...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article   label\n",
       "0  [عدل, عقل, خبر, حكيمى, حكم, جواتيمالى, ظهر, ست...  sports\n",
       "1  [سمة, جهز, طبي, زمل, حقق, باء, دول, فتر, صبة, ...  sports\n",
       "2  [طرق, قنديل, عضو, جلس, درة, ندى, اهل, نهم, يوج...  sports\n",
       "3  [حفل, ؤمن, زكر, اهل, بقز, ارد, حمر, دحل, قطر, ...  sports\n",
       "4  [وصل, بعث, فرق, ول, لكر, ندي, هلي, لى, مقر, قم...  sports"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "st = ISRIStemmer()\n",
    "\n",
    "dataset['article'] = dataset['article'].apply(lambda x : [st.stem(item) for item in x])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict_pipeline(X, y):\n",
    "    nb_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "    svc_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', LinearSVC())])\n",
    "    lr_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', LogisticRegression())])\n",
    "    rf_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', RandomForestClassifier())])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=100)\n",
    "    \n",
    "    nb_clf = nb_clf.fit(X_train, y_train)\n",
    "    predicted_nb = nb_clf.predict(X_test)\n",
    "    \n",
    "    svc_clf = svc_clf.fit(X_train, y_train)\n",
    "    predicted_svc = svc_clf.predict(X_test)\n",
    "    \n",
    "    lr_clf = lr_clf.fit(X_train, y_train)\n",
    "    predicted_lr = lr_clf.predict(X_test)\n",
    "    \n",
    "    rf_clf = rf_clf.fit(X_train, y_train)\n",
    "    predicted_rf = rf_clf.predict(X_test)\n",
    "    \n",
    "    print('Accuracy score for MNB= {:.2f}'.format(np.mean(predicted_nb == y_test)))\n",
    "    print('Accuracy score for SVC= {:.2f}'.format(np.mean(predicted_svc == y_test)))  \n",
    "    print('Accuracy score for LR= {:.2f}'.format(np.mean(predicted_lr == y_test)))\n",
    "    print('Accuracy score for RF= {:.2f}'.format(np.mean(predicted_rf == y_test)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for MNB= 0.88\n",
      "Accuracy score for SVC= 0.90\n",
      "Accuracy score for LR= 0.89\n",
      "Accuracy score for RF= 0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "train_predict_pipeline(dataset['article'].astype('str'), dataset['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Grid Search to get the best choise of parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['article'].astype('str'), dataset['label'], test_size=0.30, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'clf__loss': 'hinge', 'clf__max_iter': 100, 'clf__penalty': 'l2', 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': 10000, 'vect__ngram_range': (1, 2)}\n",
      "best scores: 0.9043873784913943\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.0),\n",
    "    'vect__max_features': (5000, 10000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  \n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__max_iter': (20, 100, 1000),\n",
    "    'clf__penalty': ('l2', 'l1'),\n",
    "    'clf__loss': ('hinge', 'squared_hinge')\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"best params: \" + str(grid_search.best_params_))\n",
    "print(\"best scores: \" + str(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.3306%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_test, grid_search.predict(X_test))\n",
    "print(\"Accuracy: {:.4%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
